from math import log
from read import Read
from common import CONFIDENCE
from time import timing
import sys

TB = 0
TP = 0 
TR = 0

def read_val_tail(
    partial_phase: tuple[dict[int, int], dict[int, int]],
    p: float,
    error: float,
    relevant_reads: list[Read],
    m: int,
    m_prev: int
) -> float:
    """
    we extend a phasing and then calculating how likely that extension is
    this looks at the prob of a particular phasing generating the set of reads 
    which cover "end" ( = "m") and for which "end" is not the smallest SNP in the read. 
    (relevant reads for the extension)
    this log prob gets added to the log prob of the non-extended phasing.
    """

    #_t = _C.seq_time()

    a = (1 - error) / (1 - (2 * error / 3.0))
    b = (error / 3.0) / (1 - (2 * error / 3.0))
    val, val2 = 0.0, 0.0 
    for read_obj in relevant_reads:
        mini_read = read_obj.mini_reads[m]
        probs, probs2 = 0.0, 0.0
        for i, pp in enumerate(partial_phase):
            prob = (CONFIDENCE * read_obj.rates[i]) + (0.5 * (1 - CONFIDENCE))
            for key in mini_read:
                prob *= (a if mini_read[key] == pp[key] else b)
            probs2 += prob / (a if mini_read[m] == pp[m] else b)
            probs += prob
        if probs == 0:
            val = -float("inf")
        else:
            val += log(probs) * read_obj.count
            if not m == read_obj.special_key:
                val2 += log(probs2) * read_obj.count

    # p is some measure of likelihood of adjacent mutations occuring together
    # since we've know extended the haplotype we need to update the prior
    if len(partial_phase[0]) > 1:
        assert m_prev != -1
        val += log(p if partial_phase[0][m] == partial_phase[0][m_prev] else 1 - p)
    
    #global TR
    #TR += _C.seq_time() - _t
    return val - val2


def branch(
    m: int,
    m_prev: int,
    lists: list[tuple[dict[int, int], dict[int, int]]],
    table: list[float],
    threshold: float,
    p: float,
    error: float,
    reads: list[Read]
):
    """
    k=2 only
    branch all solutions to highly probable solutions on one additional SNP
    lists include all current solutions
    dict has list index of solutions and their likelihoods
    """

    # partial_phase[_][m] | partial_phase[_][k for R.mini_reads[m] for R in relevant_reads]
    # relevante updates: 

    new_lists = list[tuple[dict[int, int], dict[int, int]]]()
    new_table = list[float]()
    configs = [(0, 1), (1, 0)]
    costs = [0.0, 0.0]
    threshold = log(threshold / (1 - threshold))
    for pi, partial in enumerate(lists):
        # extend solution
        for i in range(2):
            # adding possible orderings of alleles for new SNP for diploid
            partial[0][m], partial[1][m] = configs[i]
            costs[i] = read_val_tail(partial, p, error, reads, m, m_prev)
        max_cost = max(costs[0], costs[1])
        # new_costs = {key: costs[key] - max_cost for key in costs}
        used = False
        for i in range(2):
            if costs[i] - max_cost >= threshold:
                # adds extension with allele ordering c if sufficiently likely
                extended_phase = partial
                if used:
                    extended_phase = (copy(partial[0]), copy(partial[1]))
                else:
                    used = True
                extended_phase[0][m], extended_phase[1][m] = configs[i]
                new_table.append(table[pi] - costs[i])
                new_lists.append(extended_phase)
    # global TB
    # TB += _C.seq_time() - _t
    return new_lists, new_table



def prune(
    m: int,
    lists: list[tuple[dict[int, int], dict[int, int]]],
    table: list[float],
    n: int
):
    # prune solutions based on number of solutions and thresholds
    L = len(lists)
    new_lists = list[tuple[dict[int, int], dict[int, int]]]()
    new_dict = list[float]()
    go = False
    threshold = 1.0
    if m == n:
        threshold = 1.0
        go = True
    elif L > 1000:
        threshold = 0.1
    elif L > 500:
        threshold = 0.05
    elif L > 100:
        threshold = 0.01
    else:
        threshold = 0.001

    threshold = abs(log(threshold))
    min_val = min(table)
    for i, l in enumerate(lists):
        if abs(table[i] - min_val) < threshold + 0.0001:
            new_lists.append(l)
            new_dict.append(table[i])
    if go:
        new_index = 0  # random.randint(0, len(new_lists)-1)
        new_lists = [new_lists[new_index]]
        new_dict = [new_dict[new_index]]
    
    return new_lists, new_dict    


def parallel(
    start: tuple[int, int],
    threshold: float,
    p: float,
    error: float,
    read_dict: dict[int, list[Read]],
    components: dict[int, list[int]],
    output: list[tuple[int, tuple[dict[int, int], dict[int, int]]]]
):
    lists, table = [(dict[int, int](), dict[int, int]())], [0.0]
    skipped = True  # set to True to allow allele permutations
    m_prev = -1 # SEQ/TODO: check! originally None
    for m in components[start[1]]:
        if skipped:
            lists, table = branch(m, m_prev, lists, table, threshold, p, error, read_dict[m])
            lists, table = prune(m, lists, table, max(components[m]))
        else:  # specify beginning to remove allele permutations
            lists, table = [(dict[int, int](), dict[int, int]())], [0.0]
            skipped = True
        m_prev = m
    output[start[0]] = (start[1], lists[0])
    
def RNA_phase(
    threshold: float,
    p: float,
    error: float,
    read_dict: dict[int, list[Read]],
    comp_mins: list[int],
    components: dict[int, list[int]]
):
    output = [(0, (dict[int, int](), dict[int, int]())) for _ in comp_mins]
    print f'using {len(comp_mins)} workers...'
    enumerate(comp_mins) ||> parallel(threshold, p, error, read_dict, components, output)

    print f'timings: {TB/1e9} {TP/1e9} {TR/1e9}'
    return dict(output)
